{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deec3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json \n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11408554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 tasks have been successfully loaded from the DataFrame\n",
      "Filtered to 1180 tasks with valid IDs and names\n"
     ]
    }
   ],
   "source": [
    "#load raw task data from JSON \n",
    "try: \n",
    "    with open('koso-dogfood-export-2025-7-5-12-2.json', 'r') as fp: \n",
    "        koso_raw_data = json.load(fp)\n",
    "        #convert graph to dictionary values in DataFrame\n",
    "\n",
    "    PROJECT_ID = 'CZVDD94wT5KrFwAv1hhejg' \n",
    "   \n",
    "    if koso_raw_data.get('projectId') == PROJECT_ID: \n",
    "        tasks_df = pd.DataFrame(koso_raw_data['graph'].values())\n",
    "        print(f'{len(tasks_df)} tasks have been successfully loaded from the DataFrame')\n",
    "\n",
    "\n",
    "        tasks_df = tasks_df.dropna(subset=['id', 'name']).reset_index(drop=True)\n",
    "        print(f'Filtered to {len(tasks_df)} tasks with valid IDs and names')\n",
    "\n",
    "    else: \n",
    "        print(f'Skipping file: Project ID \"{koso_raw_data.get('project_id')}\" does not match target ID \"{PROJECT_ID}\".')\n",
    "        tasks_df = pd.DataFrame()\n",
    "except FileNotFoundError: \n",
    "    print(\"Error: File not found. Please ensure it's in the same directory\")\n",
    "    exit()\n",
    "except json.JSONDecodeError: \n",
    "    print('Error: Could not decode JSON. Check file format')\n",
    "    exit()\n",
    "except KeyError: \n",
    "    print(\"Error: 'graph' key not found in the JSON data. Ensure JSON structure is as expected.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57df14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mock embeddings and clustering data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['id', 'name'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.random.rand(size) * (hash_value % \u001b[32m1000\u001b[39m) / \u001b[32m1000.0\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mGenerating mock embeddings and clustering data...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m sdf = \u001b[43mtasks_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m     11\u001b[39m sdf[\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m] = sdf[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: create_mock_embedding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/koso/dedupe/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/koso/dedupe/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/koso/dedupe/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['id', 'name'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#practice embeddings to run code \n",
    "def create_mock_embedding(text, size=10): \n",
    "    #using a hash to create a unique, consistent 'embedding'\n",
    "    hash_value = int(hashlib.sha256(text.encode('utf-8')).hexdigest(), 16)\n",
    "    #creating a fake vectore based on the hash \n",
    "    return np.random.rand(size) * (hash_value % 1000) / 1000.0\n",
    "\n",
    "print('Generating mock embeddings and clustering data...')\n",
    "\n",
    "sdf = tasks_df[['id', 'name']].copy()\n",
    "sdf['embedding'] = sdf['name'].apply(lambda x: create_mock_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d41399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock data generation complete. Procceeding with deduplication\n"
     ]
    }
   ],
   "source": [
    "#Create mock cluster IDs and similarities\n",
    "\n",
    "unique_names = tasks_df['name'].unique()\n",
    "name_to_cluster = {name: i % 5 for i, name in enumerate(unique_names)} #assings 5 mock cluster\n",
    "name_to_similarity = {name: 0.85 + (int(hashlib.sha256(name.encode('utf-8')).hexdigest()[-2:], 16) / 255) * 0.14 for name in unique_names} # Simulate varying similarities\n",
    "\n",
    "jdf = tasks_df[['id', 'name']].copy()\n",
    "jdf['cluster'] = jdf['name'].map(name_to_cluster)\n",
    "jdf['similarity'] = jdf['name'].map(name_to_similarity)\n",
    "\n",
    "print('Mock data generation complete. Procceeding with deduplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19483e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying potiental duplicate pairs for frontend presentation...\n"
     ]
    }
   ],
   "source": [
    "#define similarity thresholds\n",
    "AUTO_APPROVE_THRESHOLD = 0.95\n",
    "\n",
    "PROMPT_THRESHOLD =  0.85\n",
    "\n",
    "#list to store al potiental duplicate pairs that will be sent to the frontend\n",
    "potiental_duplicate_pairs = []\n",
    "\n",
    "#set to keep track of pairs already checked to avoid redundant comparisons \n",
    "checked_pairs = set()\n",
    "\n",
    "print('Identifying potiental duplicate pairs for frontend presentation...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59f889d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#group tasks by assigend cluster\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cluster_id, group \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjdf\u001b[49m.groupby(\u001b[33m'\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m'\u001b[39m): \n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cluster_id == -\u001b[32m1\u001b[39m: \n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m#skips outlier\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'jdf' is not defined"
     ]
    }
   ],
   "source": [
    "#group tasks by assigend cluster\n",
    "\n",
    "for cluster_id, group in jdf.groupby('cluster'): \n",
    "    if cluster_id == -1: \n",
    "        continue #skips outlier\n",
    "\n",
    "    group_records = group.sort_values(by= 'similarity', ascending=False).to_dict('records')\n",
    "\n",
    "    #compare every unique pair of tasks within the current cluster \n",
    "    for i in range(len(group_records)): \n",
    "        for j in range(i + 1, len(group_records)): \n",
    "            task1_id = group_records[i]['id']\n",
    "            task2_id = group_records[j]['id']\n",
    "            task1_name = group_records[i]['name']\n",
    "            task2_name = group_records[j]['name']\n",
    "\n",
    "        #this makes (1, 2) same as (2, 1), etc\n",
    "            pair_key = tuple(sorted([task1_name, task2_name]))\n",
    "\n",
    "\n",
    "            if pair_key in checked_pairs: \n",
    "                continue \n",
    "            checked_pairs.add(pair_key) #mark pair as checked \n",
    "\n",
    "\n",
    "            if task1_id not in sdf['id'].values or task2_id not in sdf['id'].values:\n",
    "                print(f'Warning: Missing embedding for ID {task1_id} or {task2_id}. Skipping Pair.')\n",
    "                continue\n",
    "\n",
    "            #retrieves embeddings for the two tasks directly from 'sdf'\n",
    "            emb1_raw = sdf.loc[sdf['id'] == task1_id, 'embedding'].values[0]\n",
    "            emb2_raw = sdf.loc[sdf['id'] == task2_id, 'embedding'].values[0]\n",
    "\n",
    "            if not isinstance(emb1_raw, np.ndarray) or not isinstance (emb2_raw, np.ndarray):\n",
    "                print(f\"Skipping pair due to non-ndarray embedding type for: '{task1_name}' ({task1_id}) and '{task2_name}' ({task2_id}).\")\n",
    "                continue \n",
    "\n",
    "            emb1 = emb1_raw\n",
    "            emb2 = emb2_raw\n",
    "\n",
    "        \n",
    "            #calculate cosine similarity between two emebddings\n",
    "            sim = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "\n",
    "\n",
    "            if sim >= PROMPT_THRESHOLD: \n",
    "                potiental_duplicate_pairs.append({\n",
    "                    'task1_id': task1_id, \n",
    "                    'task1_name': task1_name, \n",
    "                    'task2_id': task2_id, \n",
    "                    'task2_name': task2_name, \n",
    "                    'similarity': sim, \n",
    "                    'auto_approve_candidate': sim >= AUTO_APPROVE_THRESHOLD\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec5ee1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No potential duplicate pairs found above the PROMPT_THRESHOLD.\n"
     ]
    }
   ],
   "source": [
    "#converts list of potiental dupes into pandas DataFrame \n",
    "potiental_duplicate_pairs_df = pd.DataFrame(potiental_duplicate_pairs)\n",
    "\n",
    "if not potiental_duplicate_pairs_df.empty: \n",
    "    potiental_duplicate_pairs_df = potiental_duplicate_pairs_df.sort_values(\n",
    "        by='similarity', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nSuccessfully identified potential duplicate pairs.\")\n",
    "    print(potiental_duplicate_pairs_df)\n",
    "\n",
    "\n",
    "    #output for frontend \n",
    "    output_columns = ['task1_id', 'task1_name', 'task2_id', 'task2_name', 'similarity', 'auto_approve_candidate']\n",
    "    csv_output_df = potiental_duplicate_pairs_df[output_columns]\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_file_path = 'potential_duplicates.csv'\n",
    "    csv_output_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"\\nPotential duplicate pairs saved to {csv_file_path}\")\n",
    "else:\n",
    "    print(\"\\nNo potential duplicate pairs found above the PROMPT_THRESHOLD.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koso-dedupe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
